# -*- coding: utf-8 -*-
"""MajorProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jyO_X4KwFQOg95SMWTY2Dpag92_FCdyY

***Project Title: Enhancing Cybersecurity in IoT Networks: Effective Detection of Cyber Attacks***
---

**STEP 1: IMPORTING LIBRARIES**
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

"""**STEP 2: MOUNTING CSV FILES FROM DRIVE**"""

from google.colab import drive
drive.mount('/content/drive')

"""**STEP 3: DISPLAYING THE DATA**"""

test = pd.read_csv('/content/drive/MyDrive/MajorProject/Attack.csv')
test

data = pd.read_csv('/content/drive/MyDrive/MajorProject/environmentMonitoring.csv')
data

data1 = pd.read_csv('/content/drive/MyDrive/MajorProject/patientMonitoring.csv')
data1

test.head()

data.head()

data1.head()

"""**STEP 4: INDEXING ALL THE COLUMNS**"""

test.columns

data.columns

data1.columns

import os
path = '/content/drive/MyDrive/MajorProject/'
csvs = os.listdir(path)
csvs

import pandas as pd

data1 = pd.DataFrame()

for csv in csvs:
  print(f'---- Reading {csv} ----')
  df = pd.read_csv(path+csv)
  print(f'df.shape: {df.shape}')
  empty_cols = [col for col in df.columns if df[col].isnull().all()]
  print(f'empty_cols: {len(empty_cols)}')
  print(empty_cols)
  df.fillna(0, inplace=True)
  data1 = data1.append(df, ignore_index=True)
  print(f'data1.shape: {data1.shape}')

"""**STEP 5: REMOVING UNWANTED COLUMNS**"""

unwanted_columns = ['ip.src', 'ip.dst', 'tcp.srcport', 'tcp.dstport','mqtt.topic', 'mqtt.msg', 'tcp.payload','mqtt.clientid', 'mqtt.conflags', 'mqtt.conack.flags', 'class']

data1.drop(labels=unwanted_columns, axis=1, inplace=True)
data1.shape

data1.columns

uc1 =  ['frame.time_delta', 'tcp.flags', 'tcp.time_delta', 'tcp.connection.rst', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.hdr_len',
        'mqtt.clientid_len', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.ver', 'ip.proto', 'ip.ttl', 'label']
uc2 = ['frame.time_delta', 'tcp.time_delta', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.ver', 'label']

data1 = data1[uc2]
data1.shape

data1['label'].value_counts()

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data1['mqtt.hdrflags']= label_encoder.fit_transform(data1['mqtt.hdrflags'])
from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

data1['mqtt.hdrflags']= label_encoder.fit_transform(data1['mqtt.hdrflags'])

X_train, X_test, y_train, y_test = train_test_split( data1.drop(labels=['label'], axis=1), data1['label'], test_size=0.3, random_state=100)

from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

# from sklearn.feature_selection import SelectFromModel
# from sklearn.linear_model import LogisticRegression

# Initialize the SelectFromModel with Logistic Regression as the estimator
embeded_LR_selector= SelectFromModel(LogisticRegression(penalty="l1", solver='liblinear'), threshold='0.9*median', max_features=10)

# Fit the selector to your training data
embeded_LR_selector.fit(X_train, y_train)

# Get the selected feature mask
embeded_LR_support = embeded_LR_selector.get_support()

# Extract the selected features
selected_features = X_train.columns[embeded_LR_support]

# Print the number of selected features
print(len(selected_features), 'selected features')

# Print the names of the selected features
print('Selected features: ', selected_features.tolist())
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

# from sklearn.feature_selection import SelectFromModel
# from sklearn.linear_model import LogisticRegression

# Initialize the SelectFromModel with Logistic Regression as the estimator
embeded_LR_selector= SelectFromModel(LogisticRegression(penalty="l1", solver='liblinear'), threshold='0.9*median', max_features=10)

# Fit the selector to your training data
embeded_LR_selector.fit(X_train, y_train)

# Get the selected feature mask
embeded_LR_support = embeded_LR_selector.get_support()

# Extract the selected features
selected_features = X_train.columns[embeded_LR_support]

# Print the number of selected features
print(len(selected_features), 'selected features')

# Print the names of the selected features
print('Selected features: ', selected_features.tolist())

uc1 =  ['frame.time_delta', 'tcp.flags', 'tcp.time_delta', 'tcp.connection.rst', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.hdr_len', 'mqtt.clientid_len', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.ver', 'ip.proto', 'ip.ttl']
uc2 = ['frame.time_delta', 'tcp.time_delta', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.ver']

"""**STEP 6: IMPORTING MACHINE LEARNING ALGORITHM**"""

# Import necessary libraries and modules
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn import linear_model  # Import Linear Regression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC  # Import Support Vector Machine (SVM)

# Import metrics for evaluating model performance
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import classification_report

# Now, you can use these imported modules for building models and evaluating them.

# Import the Gaussian Naive Bayes (GaussianNB) classifier
print('-------- Applying Gaussian Naive Bayes Classifier ----------')
gnb = GaussianNB()

# Train the Gaussian Naive Bayes classifier on the training data
gnb_fit = gnb.fit(X_train, y_train)

# Use the trained model to make predictions on the test data
gnb_prediction = gnb.predict(X_test)

# At this point, gnb_prediction contains the predicted labels for the test data based on the trained GaussianNB model.

# Import the K-Nearest Neighbors (KNeighborsClassifier) classifier
print('-------- Applying K-Nearest Neighbors (KNeighborsClassifier) Classifier ----------')

# Create a K-Nearest Neighbors classifier with 5 neighbors
KNN_model = KNeighborsClassifier(n_neighbors=5)

# Train the K-Nearest Neighbors classifier on the training data
KNN_model.fit(X_train, y_train)

# Use the trained K-Nearest Neighbors model to make predictions on the test data
KNN_prediction = KNN_model.predict(X_test)

# At this point, KNN_prediction contains the predicted labels for the test data based on the trained K-Nearest Neighbors model with 5 neighbors.

# Import the Random Forest Classifier
print('-------- Applying Random Forest Classifier ----------')

# Create a Random Forest Classifier with a maximum depth of 10 and a fixed random state for reproducibility
RF = RandomForestClassifier(max_depth=10, random_state=100)

# Train the Random Forest Classifier on the training data
RF.fit(X_train, y_train)

# Use the trained Random Forest model to make predictions on the test data
RF_prediction = RF.predict(X_test)

# At this point, RF_prediction contains the predicted labels for the test data based on the trained Random Forest Classifier with a maximum depth of 10.

# Import the AdaBoost Classifier
print('-------- Applying AdaBoost Classifier ----------')

# Create an AdaBoost Classifier
AB = AdaBoostClassifier()

# Train the AdaBoost Classifier on the training data
AB.fit(X_train, y_train)

# Use the trained AdaBoost model to make predictions on the test data
AB_prediction = AB.predict(X_test)

# At this point, AB_prediction contains the predicted labels for the test data based on the trained AdaBoost Classifier.

# Import the Logistic Regression Classifier
print('-------- Applying Logistic Regression Classifier ----------')

# Create a Logistic Regression Classifier using the linear_model module
LogR = linear_model.LogisticRegression()

# Train the Logistic Regression Classifier on the training data
LogR.fit(X_train, y_train)

# Use the trained Logistic Regression model to make predictions on the test data
LogR_prediction = LogR.predict(X_test)

# At this point, LogR_prediction contains the predicted labels for the test data based on the trained Logistic Regression Classifier.

fs1 =  ['frame.time_delta', 'tcp.flags', 'tcp.time_delta', 'tcp.connection.rst', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'tcp.hdr_len', 'mqtt.clientid_len', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.ver', 'ip.proto', 'ip.ttl']
fs2 = ['frame.time_delta', 'tcp.time_delta', 'tcp.flags.ack', 'tcp.flags.push', 'tcp.flags.reset', 'mqtt.hdrflags', 'mqtt.msgtype', 'mqtt.qos', 'mqtt.retain', 'mqtt.ver']

from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn import linear_model #Linear Regression
#from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC   #SVM

from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.metrics import classification_report

print('--------Applying GaussianNB Classifier ----------')
gnb = GaussianNB()
gnb_fit = gnb.fit(X_train, y_train)
gnb_prediction = gnb.predict(X_test)

print('--------Applying KNeighborsClassifier Classifier ----------')
KNN_model = KNeighborsClassifier(n_neighbors=5)
KNN_model.fit(X_train, y_train)
KNN_prediction = KNN_model.predict(X_test)

print('--------Applying RandomForestClassifier Classifier ----------')
RF = RandomForestClassifier(max_depth=10, random_state=100)
RF.fit(X_train, y_train)
RF_prediction = RF.predict(X_test)


print('--------Applying AdaBoostClassifier Classifier ----------')
AB = AdaBoostClassifier()
AB.fit(X_train, y_train)
AB_prediction = AB.predict(X_test)


print('--------Applying LogisticRegression Classifier ----------')
#LogR = LogisticRegression()
LogR = linear_model.LogisticRegression()
LogR.fit(X_train, y_train)
LogR_prediction = LogR.predict(X_test)


print('--------Applying DecisionTree Classifier ----------')
DT = DecisionTreeClassifier(criterion = "gini", random_state = 100, max_depth=10, min_samples_leaf=5)
DT.fit(X_train, y_train)
DT_prediction = DT.predict(X_test)

"""**STEP 7: DISPLAYING ACCURACY OF EACH ALGORITHM**"""

print('-------Performance Evaluation----------')
#precision_score, recall_score, f1_score, accuracy_score
results = []
selector = 'LR'
results.append((selector,'','', '',''))
results.append(('Classifier','Accurary','Precision', 'Recall','F1-Score'))
results.append(('NB ', accuracy_score(y_test,  gnb_prediction)*100,
                precision_score(y_test,  gnb_prediction)*100,
                recall_score(y_test,  gnb_prediction)*100,
                f1_score(y_test,  gnb_prediction)*100))

results.append(('KNN ', accuracy_score(y_test, KNN_prediction)*100,
                precision_score(y_test, KNN_prediction)*100,
                recall_score(y_test, KNN_prediction)*100,
                f1_score(y_test, KNN_prediction)*100))

results.append(('RF ', accuracy_score(y_test,  RF_prediction)*100,
                precision_score(y_test,  RF_prediction)*100,
                recall_score(y_test,  RF_prediction)*100,
                f1_score(y_test,  RF_prediction)*100))

results.append(('AB ', accuracy_score(y_test,  AB_prediction)*100,
                precision_score(y_test,  AB_prediction)*100,
                recall_score(y_test,  AB_prediction)*100,
                f1_score(y_test,  AB_prediction)*100))

results.append(('LogR ', accuracy_score(y_test,  LogR_prediction)*100,
                precision_score(y_test,  LogR_prediction)*100,
                recall_score(y_test,  LogR_prediction)*100,
                f1_score(y_test,  LogR_prediction)*100))

results.append(('DT ', accuracy_score(y_test,  DT_prediction)*100,
                precision_score(y_test,  DT_prediction)*100,
                recall_score(y_test,  DT_prediction)*100,
                f1_score(y_test,  DT_prediction)*100))

print('type(results): ', type(results))
print('results: ', results)
df_results = pd.DataFrame(results)
print('type(df_results): ', type(df_results))
print('df_results: ', df_results)

import numpy as np
import matplotlib.pyplot as plt
classifier=['NB','KNN','RF','AB','LogR','DT']
accuracy=[accuracy_score(y_test,  gnb_prediction)*100,
          accuracy_score(y_test, KNN_prediction)*100,
          accuracy_score(y_test,  RF_prediction)*100,
          accuracy_score(y_test,  AB_prediction)*100,
          accuracy_score(y_test,  LogR_prediction)*100,
          accuracy_score(y_test,  DT_prediction)*100]
y_points=np.arange(len(classifier))
plt.xticks(y_points,classifier)
plt.ylabel("Accuracy in Percentages")
plt.xlabel("Classifier")
plt.title("Comparing Each Classifier Accuracy")
graph = plt.bar(y_points,accuracy)
i = 0
for p in graph:
    width = p.get_width()
    height = p.get_height()
    x, y = p.get_xy()
    plt.text(x+width/2,
             y+height*1.01,
             str(accuracy[i])+'%',
             ha='center',
             weight='bold')
    i+=1

import numpy as np
import matplotlib.pyplot as plt
classifier=['NB','KNN','RF','AB','LogR','DT']
accuracy=[accuracy_score(y_test,  gnb_prediction)*100,
          accuracy_score(y_test, KNN_prediction)*100,
          accuracy_score(y_test,  RF_prediction)*100,
          accuracy_score(y_test,  AB_prediction)*100,
          accuracy_score(y_test,  LogR_prediction)*100,
          accuracy_score(y_test,  DT_prediction)*100]
fig, ax = plt.subplots()
width = 0.75
y_points=np.arange(len(accuracy))
ax.barh(y_points,accuracy,width,color='blue')
ax.set_yticks(y_points+width/2)
ax.set_yticklabels(classifier, minor=False)
plt.ylabel("Accuracy in Percentages")
plt.xlabel("Classifier")
plt.title("Comparing Each Classifier Accuracy")
for i, v in enumerate(accuracy):
    ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
classifier=['NB','KNN','RF','AB','LogR','DT']
precision=[precision_score(y_test,  gnb_prediction)*100,
          precision_score(y_test, KNN_prediction)*100,
          precision_score(y_test,  RF_prediction)*100,
          precision_score(y_test,  AB_prediction)*100,
          precision_score(y_test,  LogR_prediction)*100,
          precision_score(y_test,  DT_prediction)*100]
fig, ax = plt.subplots()
width = 0.75
y_points=np.arange(len(precision))
ax.barh(y_points,precision,width,color='blue')
ax.set_yticks(y_points+width/2)
ax.set_yticklabels(classifier, minor=False)
plt.ylabel("Precision in Percentages")
plt.xlabel("Classifier")
plt.title("Comparing Each Classifier Precision")
for i, v in enumerate(precision):
    ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
classifier=['NB','KNN','RF','AB','LogR','DT']
recall=[recall_score(y_test,  gnb_prediction)*100,
          recall_score(y_test, KNN_prediction)*100,
          recall_score(y_test,  RF_prediction)*100,
          recall_score(y_test,  AB_prediction)*100,
          recall_score(y_test,  LogR_prediction)*100,
          recall_score(y_test,  DT_prediction)*100]
fig, ax = plt.subplots()
width = 0.75
y_points=np.arange(len(recall))
ax.barh(y_points,recall,width,color='blue')
ax.set_yticks(y_points+width/2)
ax.set_yticklabels(classifier, minor=False)
plt.ylabel("Recall in Percentages")
plt.xlabel("Classifier")
plt.title("Comparing Each Classifier Recall")
for i, v in enumerate(recall):
    ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
classifier=['NB','KNN','RF','AB','LogR','DT']
F1=[f1_score(y_test,  gnb_prediction)*100,
          f1_score(y_test, KNN_prediction)*100,
          f1_score(y_test,  RF_prediction)*100,
          f1_score(y_test,  AB_prediction)*100,
          f1_score(y_test,  LogR_prediction)*100,
          f1_score(y_test,  DT_prediction)*100]
fig, ax = plt.subplots()
width = 0.75
y_points=np.arange(len(F1))
ax.barh(y_points,F1,width,color='blue')
ax.set_yticks(y_points+width/2)
ax.set_yticklabels(classifier, minor=False)
plt.ylabel("F1-Score in Percentages")
plt.xlabel("Classifier")
plt.title("Comparing Each Classifier F1-Score")
for i, v in enumerate(F1):
    ax.text(v + 3, i + .25, str(v), color='blue', fontweight='bold')
plt.show()

import matplotlib.pyplot as plt

x_axis = ['Accurary','Precision', 'Recall','F1-Score']
points_NB = [accuracy_score(y_test,  gnb_prediction)*100,
                precision_score(y_test,  gnb_prediction)*100,
                recall_score(y_test,  gnb_prediction)*100,
                f1_score(y_test,  gnb_prediction)*100]
points_KNN = [accuracy_score(y_test, KNN_prediction)*100,
                precision_score(y_test, KNN_prediction)*100,
                recall_score(y_test, KNN_prediction)*100,
                f1_score(y_test, KNN_prediction)*100]
points_RF = [accuracy_score(y_test,  RF_prediction)*100,
                precision_score(y_test,  RF_prediction)*100,
                recall_score(y_test,  RF_prediction)*100,
                f1_score(y_test,  RF_prediction)*100]
points_AB = [accuracy_score(y_test,  AB_prediction)*100,
                precision_score(y_test,  AB_prediction)*100,
                recall_score(y_test,  AB_prediction)*100,
                f1_score(y_test,  AB_prediction)*100]
points_LogR = [accuracy_score(y_test,  LogR_prediction)*100,
                precision_score(y_test,  LogR_prediction)*100,
                recall_score(y_test,  LogR_prediction)*100,
                f1_score(y_test,  LogR_prediction)*100]
points_DT = [accuracy_score(y_test,  DT_prediction)*100,
                precision_score(y_test,  DT_prediction)*100,
                recall_score(y_test,  DT_prediction)*100,
                f1_score(y_test,  DT_prediction)*100]


plt.plot(x_axis,points_NB, label='NB')
plt.plot(x_axis,points_KNN, label='KNN')
plt.plot(x_axis,points_RF, label='RF')
plt.plot(x_axis,points_AB, label='AB')
plt.plot(x_axis,points_LogR, label='LogR')
plt.plot(x_axis,points_DT, label='DT')
plt.xlabel('Classifier')
plt.ylabel('In Percentage')
plt.legend()
plt.title('Performance Evaluation')

"""**STEP 7: DISPLAY CONFUSION MATRIX**"""

import matplotlib.pyplot as plt
from sklearn import metrics
#Confusion matrix for gnb_prediction.
print('Confusion Matrix gnb_prediction')
confusion_matrix_gnb = metrics.confusion_matrix(y_test,gnb_prediction)
cm_display_gnb = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_gnb,display_labels=[False,True])
cm_display_gnb.plot()
plt.show()
#Confusion matrix for K Nearest Neighbors Classifier
print('Confusion Matrix K Nearest Neighbors Classifier')
confusion_matrix_KNN = metrics.confusion_matrix(y_test,KNN_prediction)
cm_display_KNN = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_KNN,display_labels=[False,True])
cm_display_KNN.plot()
plt.show()
#Confusion matrix for Random Forest
print('Confusion Matrix K Random Forest')
confusion_matrix_RF = metrics.confusion_matrix(y_test,RF_prediction)
cm_display_RF = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_RF,display_labels=[False,True])
cm_display_RF.plot()
plt.show()
#Confusion matrix for AdaBoost
print('Confusion Matrix AdaBoost')
confusion_matrix_AB = metrics.confusion_matrix(y_test,AB_prediction)
cm_display_AB = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_AB,display_labels=[False,True])
cm_display_AB.plot()
plt.show()
#Confusion matrix for Logistic Regression
print('Confusion Matrix Logistic Regression')
confusion_matrix_LogR = metrics.confusion_matrix(y_test,LogR_prediction)
cm_display_LogR = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_LogR,display_labels=[False,True])
cm_display_LogR.plot()
plt.show()
#Confusion matrix for Desicion Tree
print('Confusion Matrix Desicion Tree')
confusion_matrix_DT = metrics.confusion_matrix(y_test,DT_prediction)
cm_display_DT = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix_DT,display_labels=[False,True])
cm_display_DT.plot()

plt.show()